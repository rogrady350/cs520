{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This lab creates a simple chatbot from Llama-2-7b-chat-hf. This is an open source model created by Meta (i.e., Facebook) The model is optimmized for chat.\n",
        "Gradio is used for the user interface."
      ],
      "metadata": {
        "id": "DNrZtnUyYheg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Requirements:\n",
        "\n",
        "It is necessary to add a token from Hugging Face after getting approval from Meta.\n",
        "\n",
        "\n",
        "**If possible, change runtime to a T4 GPU.**\n",
        "\n",
        "\n",
        "* Hugging Face Transformers provides us with a straightforward way to use pre-trained models.\n",
        "* PyTorch does deep learning operations.\n",
        "* Accelerate optimizes PyTorch operations, especially on a GPU."
      ],
      "metadata": {
        "id": "bWOx7lgoYHZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "aNTmMJIMYjiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hugging Face Transformers provides us with a straightforward way to use pre-trained models.\n",
        "* PyTorch does deep learning operations.\n",
        "* Accelerate optimizes PyTorch operations, especially on a GPU."
      ],
      "metadata": {
        "id": "R7nk6qGHcOK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the newest Gradio"
      ],
      "metadata": {
        "id": "RgN7c_8Yoy9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "mFjx3o4HU_83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Model & Tokenizer\n",
        "\n",
        "Here, we are loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer converts  text prompts into a sequence of vectors that the model can process."
      ],
      "metadata": {
        "id": "xmJHSjx4abta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, token=True)"
      ],
      "metadata": {
        "id": "jsBrtGpZYmcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Llama Pipeline\n",
        "\n",
        "Hugging Face transformers library provides a pipeline to handle the process of giving prompts to the model and receiving text as output.\n",
        "\n",
        "We set up a pipeline for text generation.\n",
        "\n",
        "\n",
        "*Note*: This cell takes a few minutes to run"
      ],
      "metadata": {
        "id": "cKHfNL76a6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "0AqJo1R_a9IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the `get_response()` function. This is the simplest method, but does not utilize the chat features of the model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "no_Jzncvhax2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt: str):\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "   # print(sequences)\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
        "\n"
      ],
      "metadata": {
        "id": "bT9kSzLHhZ-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_response(\"Hi, I'm Rich\")"
      ],
      "metadata": {
        "id": "oSnj5a8CZZ0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_response(\"What's my name?\")"
      ],
      "metadata": {
        "id": "3vPpgYifaRHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfbhA9DFSD2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our function  `get_response() ` basically uses the language model to generate text.**\n",
        "\n",
        "* There is no conversation history. The basic approach does not account for past interactions, making it less effective for maintaining a coherent conversation.\n",
        "* It does not use the chat features of the model.\n"
      ],
      "metadata": {
        "id": "QD3Wr8F9oGjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat prompts of Llama 2**\n",
        "\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "{{ system_prompt }}\n",
        "<</SYS>>\n",
        "\n",
        "{{ user_message }} [/INST]\n",
        "```"
      ],
      "metadata": {
        "id": "BbCnlXQ9iQIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **We need to build the Prompt**\n",
        "\n",
        "The parameters:\n",
        "- `message` is the current message we're sending\n",
        "- `history` is the history of conversation as a list of tupples `[(user_msg1, bot_msg1), (usr_msg2, bot_msg2), ...]`\n"
      ],
      "metadata": {
        "id": "0-J2F3zzWkcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful bot. Your answers are clear and concise.\n",
        "<</SYS>>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Formatting function for message and history\n",
        "def format_message(message: str, history: list, memory_limit: int = 3):\n",
        "\n",
        "    \"\"\"\n",
        "    Formats the message and history for the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        message (str): Current message to send.\n",
        "        history (list): Past conversation history.\n",
        "        memory_limit (int): Limit on how many past interactions to consider.\n",
        "\n",
        "    Returns:\n",
        "        Formatted message string\n",
        "    \"\"\"\n",
        "    # always keep len(history) <= memory_limit\n",
        "    if len(history) > memory_limit:\n",
        "        history = history[-memory_limit:]\n",
        "\n",
        "    if len(history) == 0:\n",
        "        return SYSTEM_PROMPT + f\"{message} [/INST]\"\n",
        "\n",
        "    formatted_message = SYSTEM_PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n",
        "\n",
        "    # Handle conversation history\n",
        "    for user_msg, model_answer in history[1:]:\n",
        "        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n",
        "\n",
        "    # Handle the current message\n",
        "    formatted_message += f\"<s>[INST] {message} [/INST]\"\n",
        "\n",
        "    return formatted_message"
      ],
      "metadata": {
        "id": "CiWBmmV6OWVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Responses\n",
        "\n",
        "We need the function to generate responses."
      ],
      "metadata": {
        "id": "tlbsuGikcsB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a response from the Llama model\n",
        "def get_llama_response(message: str, history: list):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates a conversational response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        message (str): User's input message.\n",
        "        history (list): Past conversation history.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response from the Llama model.\n",
        "    \"\"\"\n",
        "    query = format_message(message, history)\n",
        "    response = \"\"\n",
        "\n",
        "    sequences = llama_pipeline(\n",
        "        query,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=1024,\n",
        "    )\n",
        "\n",
        "    generated_text = sequences[0]['generated_text']\n",
        "    response = generated_text[len(query):]  # Remove the prompt from the output\n",
        "\n",
        "    print(\"Chatbot:\", response.strip())\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "PeEh17FDLzEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(get_llama_response).launch()\n"
      ],
      "metadata": {
        "id": "cHk0MVvdUF3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "The results  with an open-source model are not as good as GPT-4, but still illustrate the way LLMs work.\n",
        "\n",
        "\n",
        "\n",
        "Thanks to Kris Ograbek and Ayan Debnath for their videos and blogs.\n",
        "See also: https://www.gradio.app/guides/creating-a-chatbot-fast"
      ],
      "metadata": {
        "id": "s3lxvhWKqfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWD3HFWlr2BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8S1-Az5wU7JF"
      }
    }
  ]
}